---
title: Inference
description: Chat, completion, embedding, vision, and KV cache reuse
---

# Inference

A3S Power supports chat completion, text completion, and embedding generation through a pluggable backend system.

## Backend Architecture

```rust
#[async_trait]
pub trait Backend: Send + Sync {
    fn name(&self) -> &str;
    fn supports(&self, format: &ModelFormat) -> bool;
    async fn load(&self, manifest: &ModelManifest) -> Result<()>;
    async fn unload(&self, model_name: &str) -> Result<()>;
    async fn chat(&self, model_name: &str, request: ChatRequest)
        -> Result<Pin<Box<dyn Stream<Item = Result<ChatResponseChunk>> + Send>>>;
    async fn complete(&self, model_name: &str, request: CompletionRequest)
        -> Result<Pin<Box<dyn Stream<Item = Result<CompletionResponseChunk>> + Send>>>;
    async fn embed(&self, model_name: &str, request: EmbeddingRequest)
        -> Result<EmbeddingResponse>;
}
```

The default backend is llama.cpp (feature-gated, requires C++ toolchain). A mock backend is available for testing.

## Chat Completion

Multi-turn conversation with system prompts:

```bash
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is Rust?"},
    {"role": "assistant", "content": "Rust is a systems programming language..."},
    {"role": "user", "content": "Show me a hello world example."}
  ],
  "stream": true
}'
```

Responses stream as NDJSON (Ollama API) or SSE (OpenAI API).

## Text Completion

Raw text completion without chat formatting:

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "The quick brown fox",
  "stream": false,
  "options": {
    "temperature": 0.0,
    "num_predict": 50
  }
}'
```

## Embeddings

Generate vector embeddings for text:

```bash
# Single input
curl http://localhost:11434/api/embed -d '{
  "model": "llama3.2",
  "input": "Hello world"
}'

# Batch input
curl http://localhost:11434/api/embed -d '{
  "model": "llama3.2",
  "input": ["Hello world", "Goodbye world"]
}'
```

Response:

```json
{
  "model": "llama3.2",
  "embeddings": [[0.123, -0.456, ...], [0.789, -0.012, ...]],
  "total_duration": 123456789
}
```

## Vision / Multimodal

Models with vision support (e.g., LLaVA) accept images:

```bash
# Base64 image
curl http://localhost:11434/api/chat -d '{
  "model": "llava",
  "messages": [{
    "role": "user",
    "content": "What is in this image?",
    "images": ["iVBORw0KGgo..."]
  }]
}'

# Image URL (OpenAI format)
curl http://localhost:11434/v1/chat/completions -d '{
  "model": "llava",
  "messages": [{
    "role": "user",
    "content": [
      {"type": "text", "text": "Describe this image"},
      {"type": "image_url", "image_url": {"url": "https://example.com/photo.jpg"}}
    ]
  }]
}'
```

## Chat Templates

Power auto-detects and renders chat templates using Jinja2 (via minijinja). Templates are read from the model's GGUF metadata or can be overridden:

```bash
# Override template at runtime
a3s-power run llama3.2 --template "custom-template"
```

Supported template formats include ChatML, Llama, Mistral, Gemma, Phi, and more. The template engine handles `{% for %}`, `{% if %}`, filters, and macros.

## KV Cache Reuse

Power implements KV cache prefix matching for multi-turn conversations. When a new request shares a prefix with a previous request (e.g., same system prompt + conversation history), the cached KV state is reused instead of recomputing from scratch.

This provides significant speedup for:
- Multi-turn chat sessions
- Repeated system prompts
- Few-shot prompting with shared examples

## Auto-Loading

When a request arrives for a model that isn't loaded, Power automatically:

1. Checks if the model exists locally
2. Loads it into the backend (with GPU layer estimation)
3. Serves the request

If `max_loaded_models` is reached, the least-recently-used model is evicted first.

## Model Lifecycle

```
Pull → Store → Load → Serve → Idle → Evict/Unload
                 ↑                        │
                 └────────────────────────┘
                    (auto-load on request)
```

- Models are loaded on first request (or via `a3s-power run`)
- Idle models are unloaded after `keep_alive` duration
- LRU eviction when `max_loaded_models` is reached
- Manual unload via `a3s-power stop <model>` or `POST /api/generate` with `keep_alive: "0"`
