---
title: Sessions
description: Create sessions, send prompts, stream responses, and manage conversation history
---

import { Tab, Tabs } from 'fumadocs-ui/components/tabs';

# Sessions

Each session is bound to a workspace directory. The Agent creates sessions via `agent.session(workspace, options?)`. Sessions hold their own LLM client, conversation history, and tool context.

The generation APIs â€” `send()` and `stream()` â€” send prompts to the LLM and return responses. The agent loop handles tool execution automatically.

## Create Session

<Tabs groupId="lang" items={['Rust', 'TypeScript', 'Python']}>
<Tab value="Rust">
```rust
use a3s_code_core::{Agent, SessionOptions};

let agent = Agent::new("agent.hcl").await?;

// Default session (uses config's default model)
let session = agent.session("/my-project", None)?;

// Session with model override
let session = agent.session("/my-project", Some(
    SessionOptions::new()
        .with_model("openai/gpt-4o")
))?;
```
</Tab>
<Tab value="TypeScript">
```typescript
const { Agent } = require('@a3s-lab/code');

const agent = await Agent.create('agent.hcl');

// Default session
const session = agent.session('/my-project');

// Session with model override
const session = agent.session('/my-project', {
  model: 'openai/gpt-4o',
});
```
</Tab>
<Tab value="Python">
```python
from a3s_code import Agent

agent = Agent.create("agent.hcl")

# Default session
session = agent.session("/my-project")

# Session with model override
session = agent.session("/my-project", model="openai/gpt-4o")
```
</Tab>
</Tabs>

## SessionOptions

| Parameter | Rust | TypeScript | Python | Description |
|-----------|------|-----------|--------|-------------|
| Model override | `with_model("provider/model")` | `model: "provider/model"` | `model="provider/model"` | Format: `provider/model` (e.g., `openai/gpt-4o`) |
| Skill dirs | `with_skill_dir("path")` | `skillDirs: ["path"]` | `skill_dirs=["path"]` | Extra directories to scan for skill files (merged with global `skill_dirs`) |
| Agent dirs | `with_agent_dir("path")` | `agentDirs: ["path"]` | `agent_dirs=["path"]` | Extra directories to scan for agent files (merged with global `agent_dirs`) |
| Parse retries | `with_parse_retries(n)` | `maxParseRetries: n` | `max_parse_retries=n` | Max consecutive malformed-tool-args errors before abort (default: 2) |
| Tool timeout | `with_tool_timeout(ms)` | `toolTimeoutMs: ms` | `tool_timeout_ms=ms` | Per-tool execution timeout in ms; timeout â†’ error fed back to LLM (default: none) |
| Circuit breaker | `with_circuit_breaker(n)` | `circuitBreakerThreshold: n` | `circuit_breaker_threshold=n` | Max LLM API failures before abort in non-streaming mode (default: 3) |
| Resilience bundle | `with_resilience_defaults()` | â€” | â€” | Enables parse_retries=2, tool_timeout=120s, circuit_breaker=3 |
| Sandbox | `with_sandbox(SandboxConfig)` | â€” | â€” | Route `bash` tool through A3S Box MicroVM (requires `sandbox` feature) |

The model must be defined in the agent's config file under `providers`. The format is `provider_name/model_id`.

Global `skill_dirs` and `agent_dirs` are set in the agent config. Per-session dirs merge with the global ones â€” see [Skills](/docs/code/skills) for details.

## Error Recovery & Resilience

Three layers of error recovery protect long-running sessions:

**4.1 Parse Error Recovery** â€” When the LLM returns malformed tool arguments (`__parse_error`), the error is fed back as a tool result so the model can self-correct. After `max_parse_retries` consecutive failures the loop aborts.

**4.2 Tool Execution Timeout** â€” Each tool call is wrapped in `tokio::time::timeout`. A timed-out tool produces an error message that is fed back to the LLM; the session continues.

**4.3 Circuit Breaker** â€” Transient LLM API failures (network errors, rate limits) are retried up to `circuit_breaker_threshold` times in non-streaming mode with short exponential backoff. In streaming mode any failure is fatal (events cannot be replayed).

```rust
// Rust â€” individual controls
let session = agent.session(".", Some(
    SessionOptions::new()
        .with_parse_retries(3)          // bail after 3 consecutive parse errors
        .with_tool_timeout(30_000)      // 30s per tool
        .with_circuit_breaker(5)        // retry LLM up to 5 times
))?;

// Rust â€” sensible bundle (parse=2, timeout=2min, circuit_breaker=3)
let session = agent.session(".", Some(
    SessionOptions::new().with_resilience_defaults()
))?;
```

## Send (Non-Streaming)

<Tabs groupId="lang" items={['Rust', 'TypeScript', 'Python']}>
<Tab value="Rust">
```rust
let result = session.send("What files handle authentication?").await?;
println!("{}", result.text);
println!("Tools: {}, Tokens: {}", result.tool_calls_count, result.usage.total_tokens);
```
</Tab>
<Tab value="TypeScript">
```typescript
const result = await session.send('What files handle authentication?');
console.log(result.text);
console.log(`Tools: ${result.toolCallsCount}, Tokens: ${result.totalTokens}`);
```
</Tab>
<Tab value="Python">
```python
result = session.send("What files handle authentication?")
print(result.text)
print(f"Tools: {result.tool_calls_count}, Tokens: {result.total_tokens}")
```
</Tab>
</Tabs>

## Send with Attachments (Vision)

Send image attachments alongside text prompts. Requires a vision-capable model (Claude Sonnet, GPT-4o).

<Tabs groupId="lang" items={['Rust', 'TypeScript', 'Python']}>
<Tab value="Rust">
```rust
use a3s_code_core::Attachment;

// From file (auto-detects media type from extension)
let image = Attachment::from_file("screenshot.png")?;

// Or from bytes
let image = Attachment::jpeg(raw_bytes);

let result = session.send_with_attachments(
    "What's in this screenshot?",
    &[image],
    None,
).await?;
println!("{}", result.text);
```
</Tab>
<Tab value="TypeScript">
```typescript
const image = await fs.readFile('screenshot.png');
const result = await session.sendWithAttachments(
  "What's in this screenshot?",
  [{ data: image, mediaType: 'image/png' }],
);
console.log(result.text);
```
</Tab>
<Tab value="Python">
```python
from a3s_code import Attachment

image = Attachment.from_file("screenshot.png")
result = session.send_with_attachments(
    "What's in this screenshot?",
    [image],
)
print(result.text)
```
</Tab>
</Tabs>

Supported image formats: JPEG, PNG, GIF, WebP.

Streaming variant:

```rust
let (rx, handle) = session.stream_with_attachments(
    "Describe this diagram",
    &[Attachment::from_file("diagram.png")?],
    None,
).await?;
```

### Tool Image Output

Tools can return images alongside text output. When a tool returns images, they are included as multi-modal content blocks in the tool result message sent to the LLM.

```rust
// In a custom tool implementation
async fn execute(&self, args: &Value, ctx: &ToolContext) -> Result<ToolOutput> {
    let screenshot_bytes = take_screenshot().await?;
    Ok(ToolOutput::success("Screenshot captured")
        .with_images(vec![Attachment::png(screenshot_bytes)]))
}
```

## Stream

<Tabs groupId="lang" items={['Rust', 'TypeScript', 'Python']}>
<Tab value="Rust">
```rust
use a3s_code_core::AgentEvent;

// AgentEvent is #[non_exhaustive] â€” always include a wildcard arm
let (mut rx, _handle) = session.stream("Refactor the auth module").await?;
while let Some(event) = rx.recv().await {
    match event {
        AgentEvent::TextDelta { text } => print!("{text}"),
        AgentEvent::ToolStart { name, .. } => println!("\nðŸ”§ {name}"),
        AgentEvent::End { text, usage } => {
            println!("\nâœ… Done: {} tokens", usage.total_tokens);
            break;
        }
        _ => {} // required: AgentEvent is #[non_exhaustive]
    }
}
```
</Tab>
<Tab value="TypeScript">
```typescript
// Returns an EventStream â€” use for await...of or call .next() manually
const stream = await session.stream('Refactor the auth module');
for await (const event of stream) {
  if (event.type === 'text_delta') process.stdout.write(event.text);
  if (event.type === 'tool_start') console.log(`\nðŸ”§ ${event.toolName}`);
  if (event.type === 'tool_end') console.log(`  â†’ ${event.toolOutput?.slice(0, 100)}`);
}

// Or iterate manually with .next()
const stream2 = await session.stream('Explain src/main.rs');
while (true) {
  const { value, done } = await stream2.next();
  if (done) break;
  if (value.type === 'text_delta') process.stdout.write(value.text);
}
```
</Tab>
<Tab value="Python">
```python
# Sync iteration (works without an event loop)
for event in session.stream("Refactor the auth module"):
    if event.event_type == "text_delta":
        print(event.text, end="", flush=True)
    elif event.event_type == "tool_start":
        print(f"\nðŸ”§ {event.tool_name}")
    elif event.event_type == "tool_end":
        print(f"  â†’ {event.tool_output[:100]}")

# Async iteration (inside async def)
async for event in session.stream("Refactor the auth module"):
    if event.event_type == "text_delta":
        print(event.text, end="", flush=True)
    elif event.event_type == "end":
        print(f"\nDone â€” {event.total_tokens} tokens")
        break
```
</Tab>
</Tabs>

## Conversation History (Rust)

Maintain multi-turn conversations by passing history:

```rust
use a3s_code_core::llm::{ContentBlock, Message};

let history = vec![
    Message::user("What's in src/?"),
    Message {
        role: "assistant".to_string(),
        content: vec![ContentBlock::Text {
            text: "The src/ directory contains main.rs and lib.rs.".to_string(),
        }],
        reasoning_content: None,
    },
];

// Continue the conversation
let result = session.send_with_history(&history, "Now explain main.rs").await?;
```

## Direct Tool Execution

Call tools directly without going through the LLM:

<Tabs groupId="lang" items={['Rust', 'TypeScript', 'Python']}>
<Tab value="Rust">
```rust
session.read_file("src/main.rs").await?;
session.bash("cargo test").await?;
session.glob("**/*.rs").await?;
session.grep("fn main").await?;
session.tool("write", serde_json::json!({"file_path": "x.rs", "content": "..."})).await?;
```
</Tab>
<Tab value="TypeScript">
```typescript
await session.readFile('src/main.rs');
await session.bash('cargo test');
await session.glob('**/*.rs');
await session.grep('fn main');
await session.tool('write', { file_path: 'x.rs', content: '...' });
```
</Tab>
<Tab value="Python">
```python
session.read_file("src/main.rs")
session.bash("cargo test")
session.glob("**/*.rs")
session.grep("fn main")
session.tool("write", {"file_path": "x.rs", "content": "..."})
```
</Tab>
</Tabs>

## Configuration

Full HCL configuration reference:

```hcl
# === LLM (required) ===
default_model = "anthropic/claude-sonnet-4-20250514"

# === Agent Behavior ===
max_tool_rounds  = 20          # default: 50
thinking_budget  = 4096        # reasoning token budget

# === Extensions ===
skill_dirs = ["./skills"]      # *.md skill files
agent_dirs = ["./agents"]      # *.yaml/*.md agent files

# === Storage ===
storage_backend = "file"       # "memory" | "file" | "custom"
sessions_dir    = "/tmp/a3s"   # session persistence path
storage_url     = "redis://localhost:6379"

# === Providers ===
providers {
  name    = "anthropic"
  api_key = "sk-ant-..."

  models {
    id          = "claude-sonnet-4-20250514"
    name        = "Claude Sonnet 4"
    family      = "claude-sonnet"
    tool_call   = true
    temperature = true
    reasoning   = false
    cost {
      input       = 3.0
      output      = 15.0
      cache_read  = 0.3
      cache_write = 3.75
    }
    limit {
      context = 200000
      output  = 8192
    }
  }
}

providers {
  name    = "openai"
  api_key = "sk-..."

  models {
    id        = "gpt-4o"
    name      = "GPT-4o"
    tool_call = true
  }

  models {
    id        = "gpt-4o-proxy"
    name      = "GPT-4o (via Proxy)"
    api_key   = "sk-proxy-key..."                 # per-model override
    base_url  = "https://proxy.example.com/v1"    # per-model override
    tool_call = true
  }
}
```

Provider-level `api_key` and `base_url` are defaults. Model-level values override them. See [Providers](/docs/code/providers) for details.

### Config Options

| Field | HCL | JSON | Type | Default |
|-------|-----|------|------|---------|
| Default model | `default_model` | `defaultModel` | `string` | â€” (required, `provider/model` format) |
| Max tool rounds | `max_tool_rounds` | `maxToolRounds` | `int?` | `50` |
| Thinking budget | `thinking_budget` | `thinkingBudget` | `int?` | `null` |
| Skill dirs | `skill_dirs` | `skillDirs` | `string[]` | `[]` |
| Agent dirs | `agent_dirs` | `agentDirs` | `string[]` | `[]` |
| Storage backend | `storage_backend` | `storageBackend` | `string` | `"file"` |
| Sessions dir | `sessions_dir` | `sessionsDir` | `string?` | `null` |
| Storage URL | `storage_url` | `storageUrl` | `string?` | `null` |

## Sandbox Integration

Route `bash` tool commands through an A3S Box MicroVM for isolated execution. Requires the `sandbox` Cargo feature.

**5.1 Transparent Routing** â€” When `SandboxConfig` is set, the `bash` tool silently uses the sandbox:

```rust
use a3s_code_core::{SessionOptions, SandboxConfig};

let session = agent.session(".", Some(
    SessionOptions::new().with_sandbox(SandboxConfig {
        image: "ubuntu:22.04".into(),
        memory_mb: 512,
        network: false,
        ..SandboxConfig::default()
    })
))?;

// bash commands run inside the MicroVM sandbox
let result = session.send("Run the test suite", None).await?;
```

**5.2 Explicit `sandbox` Tool** â€” With the `sandbox` feature enabled, a `sandbox` built-in tool is registered so the LLM can choose to use it explicitly. The workspace is mounted at `/workspace` inside the sandbox.

| `SandboxConfig` Field | Type | Default | Description |
|----------------------|------|---------|-------------|
| `image` | `string` | `"alpine:latest"` | OCI image reference |
| `memory_mb` | `u32` | `512` | Memory limit in megabytes |
| `cpus` | `u32` | `1` | Number of vCPUs |
| `network` | `bool` | `false` | Enable outbound networking |
| `env` | `HashMap<String,String>` | `{}` | Environment variables to inject |

Enable the feature in `Cargo.toml`:

```toml
a3s-code-core = { version = "0.7", features = ["sandbox"] }
```

## Return Types

### AgentResult

| Field | Type | Description |
|-------|------|-------------|
| `text` | `string` | Final LLM response text |
| `messages` | `Vec<Message>` | Full conversation history (Rust only) |
| `usage` | `TokenUsage` | Token usage statistics |
| `tool_calls_count` | `usize` | Number of tool calls |

### TokenUsage

| Field | Type | Description |
|-------|------|-------------|
| `prompt_tokens` | `usize` | Input tokens |
| `completion_tokens` | `usize` | Output tokens |
| `total_tokens` | `usize` | Total tokens |
| `cache_read_tokens` | `Option<usize>` | Cached input tokens read |
| `cache_write_tokens` | `Option<usize>` | Cached input tokens written |

### AgentEvent

`AgentEvent` is `#[non_exhaustive]` â€” always include a wildcard arm when matching in Rust.

**Agent lifecycle:**

| Event | Fields | Description |
|-------|--------|-------------|
| `Start` | `prompt` | Processing started |
| `TurnStart` | `turn` | New turn started |
| `TextDelta` | `text` | Text chunk from assistant |
| `TurnEnd` | `turn`, `total_tokens` | Turn completed |
| `End` | `text`, `total_tokens` | Generation finished |
| `Error` | `error` | Error occurred |

**Tool execution:**

| Event | Fields | Description |
|-------|--------|-------------|
| `ToolStart` | `tool_id`, `tool_name` | Tool call started |
| `ToolEnd` | `tool_id`, `tool_name`, `tool_output`, `exit_code` | Tool call completed |
| `ToolOutputDelta` | `tool_id`, `tool_name`, `text` | Tool output increment |

**HITL and permissions:**

| Event | Fields | Description |
|-------|--------|-------------|
| `ConfirmationRequired` | `tool_id`, `tool_name`, `args`, `timeout_ms` | HITL confirmation needed |
| `ConfirmationReceived` | `tool_id`, `approved`, `reason` | HITL confirmation received |
| `ConfirmationTimeout` | `tool_id`, `action_taken` | HITL confirmation timed out |
| `PermissionDenied` | `tool_id`, `tool_name`, `args`, `reason` | Tool blocked by permission policy |

**Context and memory:**

| Event | Fields | Description |
|-------|--------|-------------|
| `ContextResolving` | `providers` | Context resolution started |
| `ContextResolved` | `total_items`, `total_tokens` | Context resolution completed |
| `MemoryStored` | â€” | Memory item stored |
| `MemoryRecalled` | â€” | Memory item recalled |
| `MemoriesSearched` | â€” | Memory search completed |
| `MemoryCleared` | â€” | Memory cleared |

**Tasks, subagents, and lane queue:**

| Event | Fields | Description |
|-------|--------|-------------|
| `TaskUpdated` | `session_id`, `tasks` | Task list changed |
| `SubagentStart` | â€” | Subagent execution started |
| `ExternalTaskPending` | â€” | External task queued |
| `ExternalTaskCompleted` | â€” | External task finished |
| `CommandDeadLettered` | â€” | Lane command dead-lettered |
| `CommandRetry` | â€” | Lane command retried |
| `QueueAlert` | â€” | Queue alert emitted |
