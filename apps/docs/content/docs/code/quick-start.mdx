---
title: Quick Start
description: Install and run A3S Code in minutes — native library or gRPC server
---

import { Tab, Tabs } from 'fumadocs-ui/components/tabs';
import { Callout } from 'fumadocs-ui/components/callout';

# Quick Start

A3S Code runs in two modes: as a native library embedded in your app, or as a standalone gRPC server.

## Installation

### Native Library (Recommended)

<Tabs groupId="lang" items={['Rust', 'Python', 'Node.js']}>
<Tab value="Rust">
```toml
[dependencies]
a3s-code-core = "0.6"
tokio = { version = "1", features = ["full"] }
```
</Tab>
<Tab value="Python">
```bash
pip install a3s-code
```
</Tab>
<Tab value="Node.js">
```bash
npm install @a3s-lab/code
```
</Tab>
</Tabs>

### gRPC Server

```bash
# Homebrew (macOS / Linux) — prebuilt binary
brew tap a3s-lab/tap
brew install a3s-code

# Cargo (builds from source)
cargo install a3s-code

# Self-update
a3s-code update
```

## Native Mode — Hello World

No server needed. Create an agent and start coding.

<Tabs groupId="lang" items={['Rust', 'Python', 'Node.js']}>
<Tab value="Rust">
```rust
use a3s_code_core::{Agent, AgentEvent};

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let agent = Agent::builder()
        .model("claude-sonnet-4-20250514")
        .api_key(std::env::var("ANTHROPIC_API_KEY")?)
        .workspace(".")
        .build()
        .await?;

    // Non-streaming
    let result = agent.send("What does this project do?").await?;
    println!("{}", result.text);

    // Streaming
    let (mut rx, handle) = agent.stream("List all source files").await?;
    while let Some(event) = rx.recv().await {
        if let AgentEvent::TextDelta { text } = event {
            print!("{text}");
        }
    }
    handle.await??;

    // Direct tool calls
    let files = agent.glob("**/*.rs").await?;
    println!("Found {} Rust files", files.len());

    let output = agent.bash("cargo test --lib 2>&1 | tail -5").await?;
    println!("{output}");

    Ok(())
}
```
</Tab>
<Tab value="Python">
```python
import os
from a3s_code import Agent

agent = Agent(
    model="claude-sonnet-4-20250514",
    api_key=os.environ["ANTHROPIC_API_KEY"],
    workspace=".",
)

# Non-streaming
result = agent.send("What does this project do?")
print(result.text)

# Streaming
for event in agent.stream("List all source files"):
    if event.event_type == "text_delta":
        print(event.text, end="", flush=True)

# Direct tool calls
files = agent.glob("**/*.py")
print(f"\nFound {len(files)} Python files")

output = agent.bash("python -m pytest --co -q 2>&1 | tail -5")
print(output)
```
</Tab>
<Tab value="Node.js">
```typescript
import { Agent } from '@a3s-lab/code';

const agent = new Agent({
  model: 'claude-sonnet-4-20250514',
  apiKey: process.env.ANTHROPIC_API_KEY!,
  workspace: '.',
});

// Non-streaming
const result = await agent.send('What does this project do?');
console.log(result.text);

// Streaming
const events = await agent.stream('List all source files');
for (const event of events) {
  if (event.type === 'text_delta') process.stdout.write(event.text);
}

// Direct tool calls
const files = await agent.glob('**/*.ts');
console.log(`\nFound ${files.length} TypeScript files`);

const output = await agent.bash('npm test 2>&1 | tail -5');
console.log(output);
```
</Tab>
</Tabs>

## Server Mode

For multi-session management, run the gRPC server and connect with SDK clients.

### Run the Server

```bash
# Start with default settings
a3s-code

# With config file
a3s-code --config ~/.a3s/config.json

# With JSON structured logging
a3s-code --json-log

# With custom listen address and OTLP tracing
a3s-code --listen-addr 0.0.0.0:4088 --otlp-endpoint http://localhost:4317
```

### CLI Options

| Flag | Env Var | Default | Description |
|------|---------|---------|-------------|
| `-c, --config` | `A3S_CONFIG` | — | Path to config.json file |
| `-l, --listen-addr` | `LISTEN_ADDR` | `0.0.0.0:4088` | gRPC server listen address |
| `--otlp-endpoint` | `OTEL_EXPORTER_OTLP_ENDPOINT` | — | OpenTelemetry OTLP endpoint |
| `--json-log` | `A3S_LOG_FORMAT` | `false` | Output logs in JSON format |

### Connect with SDK

<Tabs groupId="lang" items={['TypeScript', 'Python']}>
<Tab value="TypeScript">
```typescript
import { A3sClient, createProvider } from '@a3s-lab/code';

const client = new A3sClient({ address: 'localhost:4088' });
const anthropic = createProvider({ name: 'anthropic', apiKey: 'sk-ant-...' });

await using session = await client.createSession({
  model: anthropic('claude-sonnet-4-20250514'),
  workspace: '/project',
});

for await (const event of session.stream('What does this project do?')) {
  if (event.textDelta) process.stdout.write(event.textDelta);
}
```
</Tab>
<Tab value="Python">
```python
from a3s_code import A3sClient, create_provider

anthropic = create_provider(name="anthropic", api_key="sk-ant-...")

async with A3sClient(address="localhost:4088") as client:
    async with await client.session(
        model=anthropic("claude-sonnet-4-20250514"),
        workspace="/project",
    ) as session:
        async for event in session.stream("What does this project do?"):
            if event.type == "text":
                print(event.content, end="", flush=True)
```
</Tab>
</Tabs>

### Server Configuration

Create `~/.a3s/config.json` (optional — LLM can also be configured per-session via SDK):

```json
{
  "defaultProvider": "anthropic",
  "defaultModel": "claude-sonnet-4-20250514",
  "providers": [
    {
      "name": "anthropic",
      "apiKey": "sk-ant-...",
      "models": [
        {
          "id": "claude-sonnet-4-20250514",
          "name": "Claude Sonnet 4",
          "family": "claude-sonnet",
          "toolCall": true,
          "temperature": true,
          "reasoning": false,
          "cost": { "input": 3.0, "output": 15.0, "cacheRead": 0.3, "cacheWrite": 3.75 },
          "limit": { "context": 200000, "output": 16384 }
        }
      ]
    },
    {
      "name": "openai",
      "apiKey": "sk-...",
      "baseUrl": "https://api.openai.com",
      "models": [
        {
          "id": "gpt-4o",
          "name": "GPT-4o",
          "family": "gpt-4o",
          "toolCall": true,
          "temperature": true,
          "cost": { "input": 2.5, "output": 10.0 },
          "limit": { "context": 128000, "output": 16384 }
        }
      ]
    }
  ],
  "storageBackend": "file",
  "sessionsDir": "~/.a3s/sessions",
  "skillDirs": ["~/.a3s/skills"],
  "agentDirs": ["~/.a3s/agents"]
}
```

### Per-Session LLM Configuration

<Tabs groupId="lang" items={['TypeScript', 'Python']}>
<Tab value="TypeScript">
```typescript
await client.configureSession(sessionId, {
  llm: {
    provider: 'openai',
    model: 'gpt-4o',
    apiKey: 'sk-...',
    baseUrl: 'https://api.openai.com',
    temperature: 0.7,
    maxTokens: 4096,
  },
  workspace: '/path/to/project',
  systemPrompt: 'You are a helpful coding assistant.',
  maxContextLength: 200000,
  autoCompact: true,
  autoCompactThreshold: 0.8,
});
```
</Tab>
<Tab value="Python">
```python
from a3s_code import SessionConfig, LLMConfig

await client.configure_session(session_id, SessionConfig(
    llm=LLMConfig(
        provider="openai",
        model="gpt-4o",
        api_key="sk-...",
        base_url="https://api.openai.com",
        temperature=0.7,
        max_tokens=4096,
    ),
    workspace="/path/to/project",
    system_prompt="You are a helpful coding assistant.",
    max_context_length=200000,
    auto_compact=True,
))
```
</Tab>
</Tabs>

## Choosing a Mode

| | Native (Library) | gRPC (Server) |
|---|---|---|
| Setup | Add dependency, no server | Install binary, start server |
| Latency | Zero IPC | Network round-trip |
| Sessions | Single agent | Multi-session with isolation |
| Features | Core loop + 11 tools + custom tools | Full 86 RPCs (HITL, cron, memory, LSP, MCP) |
| Best for | CLIs, pipelines, embedded agents | IDEs, multi-tenant services, complex workflows |
