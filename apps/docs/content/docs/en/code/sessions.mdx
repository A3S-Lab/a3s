---
title: Sessions
description: Create sessions, send prompts, stream responses, and manage conversation history
---

import { Tab, Tabs } from 'fumadocs-ui/components/tabs';
import { TypeTable } from 'fumadocs-ui/components/type-table';
import { Callout } from 'fumadocs-ui/components/callout';
import { Accordion, Accordions } from 'fumadocs-ui/components/accordion';

# Sessions

Each session is bound to a workspace directory. The Agent creates sessions via `agent.session(workspace, options?)`. Sessions hold their own LLM client, conversation history, and tool context.

The generation APIs â€” `send()` and `stream()` â€” send prompts to the LLM and return responses. The agent loop handles tool execution automatically.

## Create Session

<Tabs groupId="lang" items={['Rust', 'TypeScript', 'Python']}>
<Tab value="Rust">
```rust
use a3s_code_core::{Agent, SessionOptions};

let agent = Agent::new("agent.hcl").await?;

// Default session (uses config's default model)
let session = agent.session("/my-project", None)?;

// Session with model override
let session = agent.session("/my-project", Some(
    SessionOptions::new()
        .with_model("openai/gpt-4o")
))?;
```
</Tab>
<Tab value="TypeScript">
```typescript
const { Agent } = require('@a3s-lab/code');

const agent = await Agent.create('agent.hcl');

// Default session
const session = agent.session('/my-project');

// Session with model override
const session = agent.session('/my-project', {
  model: 'openai/gpt-4o',
});
```
</Tab>
<Tab value="Python">
```python
from a3s_code import Agent

agent = Agent.create("agent.hcl")

# Default session
session = agent.session("/my-project")

# Session with model override
session = agent.session("/my-project", model="openai/gpt-4o")
```
</Tab>
</Tabs>

## SessionOptions

<TypeTable
  type={{
    model: {
      type: 'string',
      description: 'Model override in `provider/model` format (e.g., `openai/gpt-4o`). Rust: `with_model("provider/model")`, TS: `model: "provider/model"`, Python: `model="provider/model"`',
    },
    skill_dirs: {
      type: 'string[]',
      description: 'Extra directories to scan for skill files (merged with global `skill_dirs`). Rust: `with_skill_dir("path")`, TS: `skillDirs: ["path"]`, Python: `skill_dirs=["path"]`',
    },
    agent_dirs: {
      type: 'string[]',
      description: 'Extra directories to scan for agent files (merged with global `agent_dirs`). Rust: `with_agent_dir("path")`, TS: `agentDirs: ["path"]`, Python: `agent_dirs=["path"]`',
    },
    parse_retries: {
      type: 'u32',
      default: '2',
      description: 'Max consecutive malformed-tool-args errors before abort. Rust: `with_parse_retries(n)`, TS: `maxParseRetries: n`, Python: `max_parse_retries=n`',
    },
    tool_timeout_ms: {
      type: 'Option<u64>',
      description: 'Per-tool execution timeout in ms; timeout â†’ error fed back to LLM. Rust: `with_tool_timeout(ms)`, TS: `toolTimeoutMs: ms`, Python: `tool_timeout_ms=ms`',
    },
    circuit_breaker: {
      type: 'u32',
      default: '3',
      description: 'Max LLM API failures before abort in non-streaming mode. Rust: `with_circuit_breaker(n)`, TS: `circuitBreakerThreshold: n`, Python: `circuit_breaker_threshold=n`',
    },
    resilience_defaults: {
      type: '(bundle)',
      description: 'Enables parse_retries=2, tool_timeout=120s, circuit_breaker=3. Rust: `with_resilience_defaults()`',
    },
    sandbox: {
      type: 'SandboxConfig',
      description: 'Route `bash` tool through A3S Box MicroVM (requires `sandbox` feature). Rust: `with_sandbox(SandboxConfig)`',
    },
  }}
/>

The model must be defined in the agent's config file under `providers`. The format is `provider_name/model_id`.

Global `skill_dirs` and `agent_dirs` are set in the agent config. Per-session dirs merge with the global ones â€” see [Skills](/docs/code/skills) for details.

## Error Recovery & Resilience

Three layers of error recovery protect long-running sessions:

<Accordions>
<Accordion title="Parse Error Recovery">
When the LLM returns malformed tool arguments (`__parse_error`), the error is fed back as a tool result so the model can self-correct. After `max_parse_retries` consecutive failures the loop aborts.
</Accordion>
<Accordion title="Tool Execution Timeout">
Each tool call is wrapped in `tokio::time::timeout`. A timed-out tool produces an error message that is fed back to the LLM; the session continues.
</Accordion>
<Accordion title="Circuit Breaker">
Transient LLM API failures (network errors, rate limits) are retried up to `circuit_breaker_threshold` times in non-streaming mode with short exponential backoff. In streaming mode any failure is fatal (events cannot be replayed).
</Accordion>
</Accordions>

```rust
// Rust â€” individual controls
let session = agent.session(".", Some(
    SessionOptions::new()
        .with_parse_retries(3)          // bail after 3 consecutive parse errors
        .with_tool_timeout(30_000)      // 30s per tool
        .with_circuit_breaker(5)        // retry LLM up to 5 times
))?;

// Rust â€” sensible bundle (parse=2, timeout=2min, circuit_breaker=3)
let session = agent.session(".", Some(
    SessionOptions::new().with_resilience_defaults()
))?;
```

## Send (Non-Streaming)

<Tabs groupId="lang" items={['Rust', 'TypeScript', 'Python']}>
<Tab value="Rust">
```rust
let result = session.send("What files handle authentication?").await?;
println!("{}", result.text);
println!("Tools: {}, Tokens: {}", result.tool_calls_count, result.usage.total_tokens);
```
</Tab>
<Tab value="TypeScript">
```typescript
const result = await session.send('What files handle authentication?');
console.log(result.text);
console.log(`Tools: ${result.toolCallsCount}, Tokens: ${result.totalTokens}`);
```
</Tab>
<Tab value="Python">
```python
result = session.send("What files handle authentication?")
print(result.text)
print(f"Tools: {result.tool_calls_count}, Tokens: {result.total_tokens}")
```
</Tab>
</Tabs>

## Send with Attachments (Vision)

Send image attachments alongside text prompts. Requires a vision-capable model (Claude Sonnet, GPT-4o).

<Tabs groupId="lang" items={['Rust', 'TypeScript', 'Python']}>
<Tab value="Rust">
```rust
use a3s_code_core::Attachment;

// From file (auto-detects media type from extension)
let image = Attachment::from_file("screenshot.png")?;

// Or from bytes
let image = Attachment::jpeg(raw_bytes);

let result = session.send_with_attachments(
    "What's in this screenshot?",
    &[image],
    None,
).await?;
println!("{}", result.text);
```
</Tab>
<Tab value="TypeScript">
```typescript
const image = await fs.readFile('screenshot.png');
const result = await session.sendWithAttachments(
  "What's in this screenshot?",
  [{ data: image, mediaType: 'image/png' }],
);
console.log(result.text);
```
</Tab>
<Tab value="Python">
```python
from a3s_code import Attachment

image = Attachment.from_file("screenshot.png")
result = session.send_with_attachments(
    "What's in this screenshot?",
    [image],
)
print(result.text)
```
</Tab>
</Tabs>

Supported image formats: JPEG, PNG, GIF, WebP.

Streaming variant:

```rust
let (rx, handle) = session.stream_with_attachments(
    "Describe this diagram",
    &[Attachment::from_file("diagram.png")?],
    None,
).await?;
```

### Tool Image Output

Tools can return images alongside text output. When a tool returns images, they are included as multi-modal content blocks in the tool result message sent to the LLM.

```rust
// In a custom tool implementation
async fn execute(&self, args: &Value, ctx: &ToolContext) -> Result<ToolOutput> {
    let screenshot_bytes = take_screenshot().await?;
    Ok(ToolOutput::success("Screenshot captured")
        .with_images(vec![Attachment::png(screenshot_bytes)]))
}
```

## Stream

<Tabs groupId="lang" items={['Rust', 'TypeScript', 'Python']}>
<Tab value="Rust">
```rust
use a3s_code_core::AgentEvent;

// AgentEvent is #[non_exhaustive] â€” always include a wildcard arm
let (mut rx, _handle) = session.stream("Refactor the auth module").await?;
while let Some(event) = rx.recv().await {
    match event {
        AgentEvent::TextDelta { text } => print!("{text}"),
        AgentEvent::ToolStart { name, .. } => println!("\nðŸ”§ {name}"),
        AgentEvent::End { text, usage } => {
            println!("\nâœ… Done: {} tokens", usage.total_tokens);
            break;
        }
        _ => {} // required: AgentEvent is #[non_exhaustive]
    }
}
```
</Tab>
<Tab value="TypeScript">
```typescript
// Returns an EventStream â€” use for await...of or call .next() manually
const stream = await session.stream('Refactor the auth module');
for await (const event of stream) {
  if (event.type === 'text_delta') process.stdout.write(event.text);
  if (event.type === 'tool_start') console.log(`\nðŸ”§ ${event.toolName}`);
  if (event.type === 'tool_end') console.log(`  â†’ ${event.toolOutput?.slice(0, 100)}`);
}

// Or iterate manually with .next()
const stream2 = await session.stream('Explain src/main.rs');
while (true) {
  const { value, done } = await stream2.next();
  if (done) break;
  if (value.type === 'text_delta') process.stdout.write(value.text);
}
```
</Tab>
<Tab value="Python">
```python
# Sync iteration (works without an event loop)
for event in session.stream("Refactor the auth module"):
    if event.event_type == "text_delta":
        print(event.text, end="", flush=True)
    elif event.event_type == "tool_start":
        print(f"\nðŸ”§ {event.tool_name}")
    elif event.event_type == "tool_end":
        print(f"  â†’ {event.tool_output[:100]}")

# Async iteration (inside async def)
async for event in session.stream("Refactor the auth module"):
    if event.event_type == "text_delta":
        print(event.text, end="", flush=True)
    elif event.event_type == "end":
        print(f"\nDone â€” {event.total_tokens} tokens")
        break
```
</Tab>
</Tabs>

## Conversation History (Rust)

Maintain multi-turn conversations by passing history:

```rust
use a3s_code_core::llm::{ContentBlock, Message};

let history = vec![
    Message::user("What's in src/?"),
    Message {
        role: "assistant".to_string(),
        content: vec![ContentBlock::Text {
            text: "The src/ directory contains main.rs and lib.rs.".to_string(),
        }],
        reasoning_content: None,
    },
];

// Continue the conversation
let result = session.send_with_history(&history, "Now explain main.rs").await?;
```

## Direct Tool Execution

Call tools directly without going through the LLM:

<Tabs groupId="lang" items={['Rust', 'TypeScript', 'Python']}>
<Tab value="Rust">
```rust
session.read_file("src/main.rs").await?;
session.bash("cargo test").await?;
session.glob("**/*.rs").await?;
session.grep("fn main").await?;
session.tool("write", serde_json::json!({"file_path": "x.rs", "content": "..."})).await?;
```
</Tab>
<Tab value="TypeScript">
```typescript
await session.readFile('src/main.rs');
await session.bash('cargo test');
await session.glob('**/*.rs');
await session.grep('fn main');
await session.tool('write', { file_path: 'x.rs', content: '...' });
```
</Tab>
<Tab value="Python">
```python
session.read_file("src/main.rs")
session.bash("cargo test")
session.glob("**/*.rs")
session.grep("fn main")
session.tool("write", {"file_path": "x.rs", "content": "..."})
```
</Tab>
</Tabs>

## Configuration

See [Providers & Configuration](/docs/code/providers) for the full HCL config reference including all fields, the `env()` function, queue and search configuration.

## Return Types

### AgentResult

<TypeTable
  type={{
    text: {
      type: 'string',
      description: 'Final LLM response text',
    },
    messages: {
      type: 'Vec<Message>',
      description: 'Full conversation history (Rust only)',
    },
    usage: {
      type: 'TokenUsage',
      description: 'Token usage statistics',
    },
    tool_calls_count: {
      type: 'usize',
      description: 'Number of tool calls',
    },
  }}
/>

### TokenUsage

<TypeTable
  type={{
    prompt_tokens: {
      type: 'usize',
      description: 'Input tokens',
    },
    completion_tokens: {
      type: 'usize',
      description: 'Output tokens',
    },
    total_tokens: {
      type: 'usize',
      description: 'Total tokens',
    },
    cache_read_tokens: {
      type: 'Option<usize>',
      description: 'Cached input tokens read',
    },
    cache_write_tokens: {
      type: 'Option<usize>',
      description: 'Cached input tokens written',
    },
  }}
/>

### AgentEvent

`AgentEvent` is `#[non_exhaustive]` â€” always include a wildcard arm when matching in Rust.

**Agent lifecycle:**

<TypeTable
  type={{
    Start: { type: 'prompt', description: 'Processing started' },
    TurnStart: { type: 'turn', description: 'New turn started' },
    TextDelta: { type: 'text', description: 'Text chunk from assistant' },
    TurnEnd: { type: 'turn, total_tokens', description: 'Turn completed' },
    End: { type: 'text, total_tokens', description: 'Generation finished' },
    Error: { type: 'error', description: 'Error occurred' },
  }}
/>

**Tool execution:**

<TypeTable
  type={{
    ToolStart: { type: 'tool_id, tool_name', description: 'Tool call started' },
    ToolEnd: { type: 'tool_id, tool_name, tool_output, exit_code', description: 'Tool call completed' },
    ToolOutputDelta: { type: 'tool_id, tool_name, text', description: 'Tool output increment' },
  }}
/>

**HITL and permissions:**

<TypeTable
  type={{
    ConfirmationRequired: { type: 'tool_id, tool_name, args, timeout_ms', description: 'HITL confirmation needed' },
    ConfirmationReceived: { type: 'tool_id, approved, reason', description: 'HITL confirmation received' },
    ConfirmationTimeout: { type: 'tool_id, action_taken', description: 'HITL confirmation timed out' },
    PermissionDenied: { type: 'tool_id, tool_name, args, reason', description: 'Tool blocked by permission policy' },
  }}
/>

**Context and memory:**

<TypeTable
  type={{
    ContextResolving: { type: 'providers', description: 'Context resolution started' },
    ContextResolved: { type: 'total_items, total_tokens', description: 'Context resolution completed' },
    MemoryStored: { type: 'â€”', description: 'Memory item stored' },
    MemoryRecalled: { type: 'â€”', description: 'Memory item recalled' },
    MemoriesSearched: { type: 'â€”', description: 'Memory search completed' },
    MemoryCleared: { type: 'â€”', description: 'Memory cleared' },
  }}
/>

**Tasks, subagents, and lane queue:**

<TypeTable
  type={{
    TaskUpdated: { type: 'session_id, tasks', description: 'Task list changed' },
    SubagentStart: { type: 'â€”', description: 'Subagent execution started' },
    ExternalTaskPending: { type: 'â€”', description: 'External task queued' },
    ExternalTaskCompleted: { type: 'â€”', description: 'External task finished' },
    CommandDeadLettered: { type: 'â€”', description: 'Lane command dead-lettered' },
    CommandRetry: { type: 'â€”', description: 'Lane command retried' },
    QueueAlert: { type: 'â€”', description: 'Queue alert emitted' },
  }}
/>

## API Reference

### Agent

<TypeTable
  type={{
    "new": {
      description: {`Signature: \`Agent::new(config: &str) -> Result<Agent>\` Â· Description: Load from HCL file or string`},
    },
    "from_config": {
      description: {`Signature: \`Agent::from_config(config: AgentConfig) -> Result<Agent>\` Â· Description: Load from struct`},
    },
    "session": {
      description: {`Signature: \`agent.session(workspace, options?) -> Result<AgentSession>\` Â· Description: Create a workspace-bound session`},
    },
  }}
/>

### AgentSession

<TypeTable
  type={{
    "Send prompt": {
      description: {`Rust: \`session.send(prompt, None).await?\` Â· Python: \`await session.send(prompt)\` Â· Node.js: \`await session.send(prompt)\``},
    },
    "Stream events": {
      description: {`Rust: \`session.stream(prompt, None).await?\` Â· Python: \`session.stream(prompt)\` Â· Node.js: \`session.stream(prompt)\``},
    },
    "Send with image": {
      description: {`Rust: \`session.send_with_attachments(p, &[img], None).await?\` Â· Python: \`await session.send_with_attachments(p, [img])\` Â· Node.js: \`await session.sendWithAttachments(p, [img])\``},
    },
    "Read file": {
      description: {`Rust: \`session.read_file(path).await?\` Â· Python: \`await session.read_file(path)\` Â· Node.js: \`await session.readFile(path)\``},
    },
    "Run bash": {
      description: {`Rust: \`session.bash(cmd).await?\` Â· Python: \`await session.bash(cmd)\` Â· Node.js: \`await session.bash(cmd)\``},
    },
    "Glob": {
      description: {`Rust: \`session.glob(pattern).await?\` Â· Python: \`await session.glob(pattern)\` Â· Node.js: \`await session.glob(pattern)\``},
    },
    "Grep": {
      description: {`Rust: \`session.grep(pattern).await?\` Â· Python: \`await session.grep(pattern)\` Â· Node.js: \`await session.grep(pattern)\``},
    },
    "Call tool": {
      description: {`Rust: \`session.tool(name, args).await?\` Â· Python: \`await session.tool(name, args)\` Â· Node.js: \`await session.tool(name, args)\``},
    },
    "History": {
      description: {`Rust: \`session.history()\` Â· Python: \`session.history()\` Â· Node.js: \`session.history()\``},
    },
  }}
/>

### SessionOptions

<TypeTable
  type={{
    model: {
      type: 'string',
      default: 'config default',
      description: 'Model in `provider/model` format. Rust: `.with_model(...)`, TS: `model: "..."`, Python: `model="..."`',
    },
    permissive: {
      type: 'bool',
      default: 'false',
      description: 'Allow all tools by default. Rust: `.with_permissive_policy()`, TS: `permissive: true`, Python: `permissive=True`',
    },
    planning: {
      type: 'bool',
      default: 'false',
      description: 'Enable LLM task planning. Rust: `.with_planning(true)`, TS: `planning: true`, Python: `planning=True`',
    },
    goal_tracking: {
      type: 'bool',
      default: 'false',
      description: 'Track goal progress. Rust: `.with_goal_tracking(true)`, TS: `goalTracking: true`, Python: `goal_tracking=True`',
    },
    builtin_skills: {
      type: 'bool',
      default: 'false',
      description: 'Load built-in skills. Rust: `.with_builtin_skills()`, TS: `builtinSkills: true`, Python: `builtin_skills=True`',
    },
    skill_dirs: {
      type: 'string[]',
      default: '[]',
      description: 'Directories to scan for skill files. Rust: `.with_skills_from_dir(path)`, TS: `skillDirs: [path]`, Python: `skill_dirs=[path]`',
    },
    default_security: {
      type: 'bool',
      default: 'false',
      description: 'Enable default security provider. Rust: `.with_default_security()`, TS: `defaultSecurity: true`, Python: `default_security=True`',
    },
    file_memory: {
      type: 'Option<string>',
      default: 'None',
      description: 'Path for file-backed memory. Rust: `.with_file_memory(path)`, TS: `memoryDir: path`, Python: `memory_dir=path`',
    },
    auto_compact: {
      type: 'bool',
      default: 'false',
      description: 'Enable auto context compaction. Rust: `.with_auto_compact(true)`, TS: `autoCompact: true`, Python: `auto_compact=True`',
    },
    auto_compact_threshold: {
      type: 'f32',
      default: '0.8',
      description: 'Context usage fraction to trigger compaction. Rust: `.with_auto_compact_threshold(f)`, TS: `autoCompactThreshold: f`, Python: `auto_compact_threshold=f`',
    },
    fs_context: {
      type: 'Option<string>',
      default: 'None',
      description: 'Filesystem context path for RAG. Rust: `.with_fs_context(path)`, TS: `fsContext: path`, Python: `fs_context=path`',
    },
    queue_config: {
      type: 'Option<SessionQueueConfig>',
      default: 'None',
      description: 'Lane queue configuration. Rust: `.with_queue_config(cfg)`, TS: `queueConfig: cfg`, Python: `queue_config=cfg`',
    },
    parse_retries: {
      type: 'u32',
      default: '2',
      description: 'Max consecutive parse errors before abort. Rust: `.with_parse_retries(n)`, TS: `maxParseRetries: n`, Python: `max_parse_retries=n`',
    },
    tool_timeout_ms: {
      type: 'Option<u64>',
      default: 'None',
      description: 'Per-tool execution timeout in ms. Rust: `.with_tool_timeout(ms)`, TS: `toolTimeoutMs: ms`, Python: `tool_timeout_ms=ms`',
    },
    circuit_breaker: {
      type: 'u32',
      default: '3',
      description: 'Max LLM API failures before abort. Rust: `.with_circuit_breaker(n)`, TS: `circuitBreakerThreshold: n`, Python: `circuit_breaker_threshold=n`',
    },
    sandbox: {
      type: 'Option<SandboxConfig>',
      default: 'None',
      description: 'Route `bash` through A3S Box MicroVM. Rust: `.with_sandbox(cfg)`',
    },
    permission_checker: {
      type: 'Arc<dyn PermissionChecker>',
      default: 'PermissionPolicy',
      description: 'Custom permission checker. Rust: `.with_permission_checker(p)`',
    },
    confirmation_manager: {
      type: 'Option<Arc<dyn ConfirmationManager>>',
      default: 'None',
      description: 'Custom HITL confirmation manager. Rust: `.with_confirmation_manager(m)`',
    },
    security_provider: {
      type: 'Option<Arc<dyn SecurityProvider>>',
      default: 'None',
      description: 'Custom security provider. Rust: `.with_security_provider(p)`',
    },
    context_provider: {
      type: 'Option<Arc<dyn ContextProvider>>',
      default: 'None',
      description: 'Custom context/RAG provider. Rust: `.with_context_provider(p)`',
    },
    memory: {
      type: 'Arc<dyn MemoryStore>',
      default: 'InMemoryStore',
      description: 'Memory storage backend. Rust: `.with_memory(store)`',
    },
    hook_engine: {
      type: 'HookEngine',
      default: 'HookEngine',
      description: 'Custom hook engine. Rust: `.with_hook_engine(h)`',
    },
  }}
/>

### AgentResponse

<TypeTable
  type={{
    "Text": {
      description: {`Rust: \`result.text\` Â· Python: \`result.text\` Â· Node.js: \`result.text\``},
    },
    "Total tokens": {
      description: {`Rust: \`result.usage.total_tokens\` Â· Python: \`result.usage.total_tokens\` Â· Node.js: \`result.usage.totalTokens\``},
    },
    "Tool call count": {
      description: {`Rust: \`result.tool_calls_count\` Â· Python: \`result.tool_calls_count\` Â· Node.js: \`result.toolCallsCount\``},
    },
  }}
/>
