---
title: Tool Calling
description: Function calling with structured tool definitions
---

# Tool Calling

A3S Power supports OpenAI-compatible tool/function calling. Define tools in the request and the model will emit structured tool call responses.

## Basic Tool Call

```bash
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:3b",
    "messages": [{"role": "user", "content": "What is the weather in San Francisco?"}],
    "tools": [{
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get current weather for a location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {"type": "string", "description": "City name"},
            "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
          },
          "required": ["location"]
        }
      }
    }]
  }'
```

Response:

```json
{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": null,
      "tool_calls": [{
        "id": "call_abc123",
        "type": "function",
        "function": {
          "name": "get_weather",
          "arguments": "{\"location\": \"San Francisco\", \"unit\": \"celsius\"}"
        }
      }]
    },
    "finish_reason": "tool_calls"
  }]
}
```

## Multi-Turn Tool Use

```python
from openai import OpenAI
import json

client = OpenAI(base_url="http://localhost:11434/v1", api_key="unused")

tools = [{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Get current weather",
        "parameters": {
            "type": "object",
            "properties": {"location": {"type": "string"}},
            "required": ["location"]
        }
    }
}]

messages = [{"role": "user", "content": "What's the weather in Tokyo?"}]

# First turn: model calls the tool
response = client.chat.completions.create(
    model="llama3.2:3b",
    messages=messages,
    tools=tools
)

tool_call = response.choices[0].message.tool_calls[0]
messages.append(response.choices[0].message)

# Execute the tool
result = {"temperature": 22, "condition": "sunny"}

# Second turn: provide tool result
messages.append({
    "role": "tool",
    "tool_call_id": tool_call.id,
    "content": json.dumps(result)
})

final = client.chat.completions.create(
    model="llama3.2:3b",
    messages=messages,
    tools=tools
)
print(final.choices[0].message.content)
```

## Tool Choice

```json
{
  "tool_choice": "auto",       // Model decides (default)
  "tool_choice": "none",       // Never call tools
  "tool_choice": "required",   // Must call a tool
  "tool_choice": {             // Force a specific tool
    "type": "function",
    "function": {"name": "get_weather"}
  }
}
```

## Parallel Tool Calls

Models that support it can call multiple tools in a single response:

```json
{
  "choices": [{
    "message": {
      "tool_calls": [
        {"id": "call_1", "function": {"name": "get_weather", "arguments": "{\"location\": \"Tokyo\"}"}},
        {"id": "call_2", "function": {"name": "get_weather", "arguments": "{\"location\": \"London\"}"}}
      ]
    }
  }]
}
```
