---
title: Inference
description: Chat, completion, embeddings, vision, streaming, and model backends
---

import { TypeTable } from 'fumadocs-ui/components/type-table';

# Inference

## Backends

A3S Power uses a pluggable `Backend` trait. Three implementations are available:

<TypeTable
  type={{
    "mistralrs (default)": {
      description: "Pure Rust inference via mistralrs (candle). No C++ toolchain required. Supports GGUF, SafeTensors, Vision, and Embedding formats. Ideal for TEE supply-chain auditing.",
    },
    "llamacpp (optional)": {
      description: "C++ llama.cpp via llama-cpp-2 bindings. Requires C++ compiler + CMake. Feature flag: --features llamacpp.",
    },
    "picolm (optional)": {
      description: "Pure Rust layer-streaming GGUF inference. Peak RAM = O(layer_size) not O(model_size). Enables 7B+ models in 512MB TEE EPC. Feature flag: --features picolm.",
    },
  }}
/>

## Chat Completion

```bash
# Non-streaming
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is Rust?"}
    ]
  }'

# Streaming (token-by-token SSE)
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:3b",
    "messages": [{"role": "user", "content": "Hello!"}],
    "stream": true
  }'
```

Streaming delivers each token as it is generated via SSE (`data: {...}`). The connection closes with `data: [DONE]` when generation completes.

## Text Completion

```bash
curl http://localhost:11434/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:3b",
    "prompt": "The quick brown fox",
    "max_tokens": 50
  }'
```

## Embeddings

```bash
# Single input
curl http://localhost:11434/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen3-embed", "input": "Hello world"}'

# Batch input
curl http://localhost:11434/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen3-embed", "input": ["Hello world", "Goodbye world"]}'
```

Register an embedding model with `format=huggingface` (see [Models](/docs/en/power/models)).

## Vision / Multimodal

Register a vision model with `format=vision`, then pass images in the request:

```bash
# Base64 image via images field
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llava:7b",
    "messages": [{"role": "user", "content": "What is in this image?"}],
    "images": ["iVBORw0KGgoAAAANSUhEUgAA..."]
  }'

# OpenAI-style image_url content parts
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llava:7b",
    "messages": [{
      "role": "user",
      "content": [
        {"type": "text", "text": "Describe this image"},
        {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,/9j/4AAQ..."}}
      ]
    }]
  }'
```

Supported image formats: JPEG, PNG, WebP.

## Generation Parameters

Pass these in the request body:

<TypeTable
  type={{
    temperature: { type: 'float', default: '0.8', description: 'Randomness (0 = deterministic)' },
    top_p: { type: 'float', default: '0.9', description: 'Nucleus sampling threshold' },
    top_k: { type: 'int', default: '40', description: 'Top-K sampling' },
    min_p: { type: 'float', default: '0.0', description: 'Minimum probability threshold' },
    max_tokens: { type: 'int', description: 'Max tokens to generate' },
    stop: { type: 'string[]', description: 'Stop sequences' },
    seed: { type: 'int', description: 'Random seed for reproducibility' },
    repeat_penalty: { type: 'float', default: '1.1', description: 'Repetition penalty' },
    frequency_penalty: { type: 'float', default: '0.0', description: 'Frequency-based penalty' },
    presence_penalty: { type: 'float', default: '0.0', description: 'Presence-based penalty' },
    num_ctx: { type: 'int', default: '2048', description: 'Context window size' },
    num_batch: { type: 'int', description: 'Batch size for prompt processing' },
    num_thread: { type: 'int', description: 'CPU threads' },
    num_gpu: { type: 'int', default: '-1', description: 'GPU layers to offload (-1 = all)' },
    flash_attention: { type: 'bool', default: 'false', description: 'Enable flash attention' },
    use_mlock: { type: 'bool', default: 'false', description: 'Lock model in memory' },
  }}
/>

## Chat Templates

Power renders chat templates using Jinja2 (via `minijinja`). Templates are read from GGUF metadata or can be overridden in the model manifest via `template_override`.

Supported formats: ChatML, Llama 3, Mistral, Gemma, Phi, and custom Jinja2 templates.

## KV Cache Reuse

When a new request shares a prefix with a previous request (same system prompt + conversation history), the cached KV state is reused. This provides significant speedup for multi-turn conversations and repeated system prompts.

## Thinking / Reasoning Models

DeepSeek-R1, QwQ, and other reasoning models emit `<think>...</think>` blocks. Power's streaming parser separates thinking content from the final response, forwarding each as distinct chunks.

## Auto-Loading

When a request arrives for a model that isn't loaded, Power automatically loads it. If `max_loaded_models` is reached, the least-recently-used model is evicted first.
