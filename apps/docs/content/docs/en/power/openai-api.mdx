---
title: OpenAI API
description: OpenAI-compatible API endpoint reference
---

import { TypeTable } from 'fumadocs-ui/components/type-table';

# OpenAI API

A3S Power provides a full OpenAI-compatible API. Any OpenAI SDK works out of the box by pointing `base_url` at the Power server.

## Endpoints

<TypeTable
  type={{
    "POST /v1/chat/completions": { description: "Chat completion — streaming and non-streaming, vision, tools, thinking" },
    "POST /v1/completions": { description: "Text completion — streaming and non-streaming" },
    "POST /v1/embeddings": { description: "Generate embeddings (requires huggingface format model)" },
    "GET /v1/models": { description: "List all registered models" },
    "GET /v1/models/:name": { description: "Get a single model by name" },
    "POST /v1/models": { description: "Register a local model (name, path, format)" },
    "DELETE /v1/models/:name": { description: "Unload and deregister a model" },
    "POST /v1/models/pull": { description: "Pull a model from HuggingFace Hub (SSE progress stream)" },
    "GET /v1/models/pull/:name/status": { description: "Get persisted pull progress for a model" },
    "GET /v1/attestation": { description: "TEE attestation report with optional nonce and model binding" },
    "GET /health": { description: "Health check with TEE status, version, uptime" },
    "GET /metrics": { description: "Prometheus metrics" },
  }}
/>

## Chat Completions

`POST /v1/chat/completions`

```bash
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 256,
    "stream": false
  }'
```

### Python SDK

```python
from openai import OpenAI

client = OpenAI(base_url="http://localhost:11434/v1", api_key="unused")

# Non-streaming
response = client.chat.completions.create(
    model="llama3.2:3b",
    messages=[{"role": "user", "content": "Explain Rust"}],
    temperature=0.7,
    max_tokens=512
)
print(response.choices[0].message.content)

# Streaming
stream = client.chat.completions.create(
    model="llama3.2:3b",
    messages=[{"role": "user", "content": "Count to 5"}],
    stream=True
)
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### TypeScript SDK

```typescript
import OpenAI from "openai";

const client = new OpenAI({ baseURL: "http://localhost:11434/v1", apiKey: "unused" });

const response = await client.chat.completions.create({
  model: "llama3.2:3b",
  messages: [{ role: "user", content: "Hello!" }],
});
console.log(response.choices[0].message.content);
```

## Text Completions

`POST /v1/completions`

```bash
curl http://localhost:11434/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:3b",
    "prompt": "Once upon a time",
    "max_tokens": 100
  }'
```

## Embeddings

`POST /v1/embeddings`

```bash
curl http://localhost:11434/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen3-embed", "input": ["Hello", "World"]}'
```

## Model Management

```bash
# List models
curl http://localhost:11434/v1/models

# Register a local model
curl -X POST http://localhost:11434/v1/models \
  -H "Content-Type: application/json" \
  -d '{"name": "llama3.2:3b", "path": "/models/llama3.2-q4.gguf"}'

# Delete a model
curl -X DELETE http://localhost:11434/v1/models/llama3.2:3b
```

## Pull from HuggingFace Hub

`POST /v1/models/pull` — streams SSE progress events.

```bash
curl -N http://localhost:11434/v1/models/pull \
  -H "Content-Type: application/json" \
  -d '{"name": "bartowski/Llama-3.2-3B-Instruct-GGUF:Q4_K_M"}'
```

Check pull status (persists across restarts):

```bash
curl http://localhost:11434/v1/models/pull/bartowski%2FLlama-3.2-3B-Instruct-GGUF:Q4_K_M/status
```

## Attestation

`GET /v1/attestation` — returns a TEE attestation report. Returns `503` if TEE is not enabled.

```bash
# Basic
curl http://localhost:11434/v1/attestation

# With nonce (prevents replay attacks)
curl "http://localhost:11434/v1/attestation?nonce=deadbeef01234567"

# Bind to a specific model's SHA-256
curl "http://localhost:11434/v1/attestation?model=llama3.2:3b"
```

```json
{
  "tee_type": "sev-snp",
  "report": "base64-encoded-raw-report",
  "report_data": "hex-encoded-64-bytes",
  "measurement": "hex-encoded-48-bytes",
  "timestamp": "2026-02-21T00:00:00Z"
}
```
