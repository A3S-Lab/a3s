---
title: Observability
description: Prometheus metrics, health checks, and usage dashboard
---

import { TypeTable } from 'fumadocs-ui/components/type-table';

# Observability

A3S Power provides Prometheus metrics, health checks, and a usage dashboard for monitoring inference workloads.

## Health Check

`GET /health` — Server health status.

```bash
curl http://localhost:11434/health
```

```json
{
  "status": "ok",
  "version": "0.1.4",
  "uptime_secs": 3600,
  "loaded_models": ["llama3.2"]
}
```

The root endpoint `GET /` returns `"Ollama is running"` for Ollama compatibility.

## Prometheus Metrics

`GET /metrics` — Prometheus-format metrics.

```bash
curl http://localhost:11434/metrics
```

### Request Metrics

<TypeTable
  type={
    {`\`power_requests_total\``}: {
      type: "Counter",
      description: "Total API requests by endpoint and status",
    },
    {`\`power_request_duration_seconds\``}: {
      type: "Histogram",
      description: "Request latency distribution",
    },
    {`\`power_active_requests\``}: {
      type: "Gauge",
      description: "Currently in-flight requests",
    },
  }
/>

### Inference Metrics

<TypeTable
  type={
    {`\`power_tokens_generated_total\``}: {
      type: "Counter",
      description: "Total tokens generated by model",
    },
    {`\`power_tokens_prompt_total\``}: {
      type: "Counter",
      description: "Total prompt tokens processed by model",
    },
    {`\`power_time_to_first_token_seconds\``}: {
      type: "Histogram",
      description: "Time to first token (TTFT)",
    },
    {`\`power_token_generation_seconds\``}: {
      type: "Histogram",
      description: "Per-token generation latency",
    },
  }
/>

### Model Metrics

<TypeTable
  type={
    {`\`power_models_loaded\``}: {
      type: "Gauge",
      description: "Currently loaded model count",
    },
    {`\`power_model_load_duration_seconds\``}: {
      type: "Histogram",
      description: "Model loading time",
    },
    {`\`power_model_evictions_total\``}: {
      type: "Counter",
      description: "LRU model evictions",
    },
  }
/>

### GPU Metrics

<TypeTable
  type={
    {`\`power_gpu_memory_used_bytes\``}: {
      type: "Gauge",
      description: "GPU memory in use",
    },
    {`\`power_gpu_memory_total_bytes\``}: {
      type: "Gauge",
      description: "Total GPU memory",
    },
    {`\`power_gpu_layers_offloaded\``}: {
      type: "Gauge",
      description: "Layers offloaded to GPU per model",
    },
  }
/>

### Cost Metrics

<TypeTable
  type={
    {`\`power_estimated_cost_total\``}: {
      type: "Counter",
      description: "Estimated inference cost (based on token counts)",
    },
  }
/>

## Usage Dashboard

`GET /v1/usage` — Aggregated usage statistics with filtering.

```bash
# All usage
curl http://localhost:11434/v1/usage

# Filter by date range
curl "http://localhost:11434/v1/usage?start=2024-01-01&end=2024-01-31"

# Filter by model
curl "http://localhost:11434/v1/usage?model=llama3.2"
```

Response:

```json
{
  "data": [
    {
      "model": "llama3.2",
      "requests": 1500,
      "prompt_tokens": 250000,
      "completion_tokens": 180000,
      "total_tokens": 430000,
      "estimated_cost": 0.0
    }
  ],
  "total_requests": 1500,
  "total_tokens": 430000
}
```

## Logging

Power uses the `tracing` crate for structured logging:

```bash
# Set log level via CLI
a3s-power serve --log-level debug

# Set via environment variable
RUST_LOG=a3s_power=debug a3s-power serve

# Ollama-compatible debug flag
OLLAMA_DEBUG=1 a3s-power serve
```

Log output includes:
- Model load/unload events with timing
- Request routing and completion
- Token generation statistics
- GPU detection and memory allocation
- Error details with context

## Grafana Integration

Example Grafana dashboard queries:

```txt
# Request rate (per second)
rate(power_requests_total[5m])

# Average TTFT
histogram_quantile(0.95, rate(power_time_to_first_token_seconds_bucket[5m]))

# Token throughput
rate(power_tokens_generated_total[5m])

# GPU memory utilization
power_gpu_memory_used_bytes / power_gpu_memory_total_bytes * 100

# Model eviction rate
rate(power_model_evictions_total[5m])
```
