---
title: A3S Power
description: Privacy-preserving LLM inference server for Trusted Execution Environments
---

import { TypeTable } from 'fumadocs-ui/components/type-table';

# A3S Power

A3S Power is a privacy-preserving LLM inference server built to run inside Trusted Execution Environments (TEE). It provides an OpenAI-compatible API for chat completions, text completions, embeddings, and vision — with hardware-enforced memory encryption, model integrity verification, and automatic log redaction.

## Why Power?

- **TEE-native**: Runs inside AMD SEV-SNP and Intel TDX MicroVMs with hardware-encrypted memory
- **OpenAI-compatible**: Works with any OpenAI SDK (`/v1/chat/completions`, `/v1/embeddings`)
- **Pure Rust inference**: Default backend via `mistralrs` (candle) — no C++ toolchain, ideal for supply-chain auditing
- **Privacy by default**: Prompts and responses never appear in logs; memory is zeroed on model unload
- **Verifiable**: Remote attestation proves inference runs in a genuine TEE before you trust the output

## Architecture

```
┌──────────────────────────────────────────────────────┐
│                     a3s-power                         │
│                                                       │
│  TEE Layer                                            │
│  ┌────────────────┐ ┌──────────────┐ ┌────────────┐  │
│  │  attestation   │ │  model_seal  │ │  privacy   │  │
│  │  (TeeProvider) │ │  (SHA-256)   │ │  (redact)  │  │
│  └────────┬───────┘ └──────┬───────┘ └─────┬──────┘  │
│                                                       │
│  Server Layer                                         │
│  ┌─────────────────────────────────────────────────┐  │
│  │  Axum Router  /health  /metrics  /v1/*          │  │
│  └──────────────────────┬──────────────────────────┘  │
│                         │                             │
│  Backend Layer                                        │
│  ┌──────────────────────────────────────────────────┐ │
│  │  MistralRsBackend (default) · LlamaCppBackend    │ │
│  │  PicolmBackend (TEE layer-streaming)             │ │
│  │  GGUF · SafeTensors · Vision · Embeddings        │ │
│  └──────────────────────────────────────────────────┘ │
│                                                       │
│  Model Layer                                          │
│  ┌──────────────────────────────────────────────────┐ │
│  │  ModelRegistry · SHA-256 blob store · HF pull    │ │
│  └──────────────────────────────────────────────────┘ │
└──────────────────────────────────────────────────────┘
```

## Key Components

<TypeTable
  type={{
    "TEE Layer": {
      description: "Remote attestation, model integrity (SHA-256), log redaction, memory zeroing, AES-256-GCM encrypted model loading",
    },
    "Backend Layer": {
      description: "Pluggable inference engines: mistralrs (pure Rust, default), llama.cpp (optional), and picolm (TEE layer-streaming, optional). Supports GGUF, SafeTensors, Vision, and Embedding model formats.",
    },
    "Model Layer": {
      description: "Content-addressed blob store, model registry, GGUF metadata parser, HuggingFace Hub pull with resume support",
    },
    "Server Layer": {
      description: "Axum HTTP server with rate limiting, Prometheus metrics, RA-TLS transport, Vsock transport, audit logging",
    },
    "API Layer": {
      description: "OpenAI-compatible: /v1/chat/completions, /v1/completions, /v1/embeddings, /v1/models, /v1/attestation",
    },
  }}
/>

## Model Format Support

<TypeTable
  type={{
    "GGUF": {
      description: "Quantized models (Q4_K_M, Q8_0, etc.). Loaded by both mistralrs and llama.cpp backends.",
    },
    "SafeTensors": {
      description: "HuggingFace SafeTensors chat models. Loaded via TextModelBuilder with ISQ on-load quantization.",
    },
    "Vision": {
      description: "Multimodal models (LLaVA, Phi-3-Vision). Loaded via VisionModelBuilder. Accepts base64 images.",
    },
    "HuggingFace": {
      description: "Embedding models (Qwen3-Embedding, GTE, NomicBert). Loaded via EmbeddingModelBuilder.",
    },
  }}
/>

## Default Paths

<TypeTable
  type={{
    "~/.a3s/power/": {
      description: "Base directory (override with $A3S_POWER_HOME)",
    },
    "~/.a3s/power/config.hcl": {
      description: "HCL configuration file",
    },
    "~/.a3s/power/models/manifests/": {
      description: "Model manifest JSON files",
    },
    "~/.a3s/power/models/blobs/": {
      description: "Content-addressed model blobs (SHA-256)",
    },
    "~/.a3s/power/pulls/": {
      description: "Pull progress state files (resume support)",
    },
  }}
/>
